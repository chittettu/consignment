{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e961f62-6199-4a29-945d-473cb571b491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('artifacts\\\\train.csv', 'artifacts\\\\test.csv')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from src.logger import logging\n",
    "from src.exception import shippingException\n",
    "\n",
    "\n",
    "from src.components.ingestion import DataIngestion\n",
    "from src.components.transformation import DataTransformation\n",
    "from src.components.training import ModelTrainer\n",
    "\n",
    "obj=DataIngestion()\n",
    "\n",
    "\n",
    "obj.initiate_data_ingestion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d16d8460-b30b-445f-955a-95e3a96312ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.logger import logging\n",
    "from src.exception import shippingException\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class DataIngestionConfig:\n",
    "    raw_data_path:str=os.path.join(\"artifacts\",\"raw.csv\")\n",
    "    train_data_path:str=os.path.join(\"artifacts\",\"train.csv\")\n",
    "    test_data_path:str=os.path.join(\"artifacts\",\"test.csv\")\n",
    "\n",
    "class DataIngestion:\n",
    "    def __init__(self):\n",
    "        self.ingestion_config=DataIngestionConfig()\n",
    "        \n",
    "\n",
    "    def initiate_data_ingestion(self):\n",
    "        logging.info(\"data ingestion started\")\n",
    "        try:\n",
    "            data=pd.read_csv(r'E:\\data_science\\project\\Internship\\SCMS_Delivery_History_Dataset.csv')\n",
    "            data.columns=data.columns.str.lower()\n",
    "            data.columns=data.columns.str.replace(\" \",\"_\")\n",
    "            logging.info(\" reading a df\")\n",
    "\n",
    "            os.makedirs(os.path.dirname(os.path.join(self.ingestion_config.raw_data_path)),exist_ok=True)\n",
    "            data.to_csv(self.ingestion_config.raw_data_path,index=False)\n",
    "            logging.info(\" i have saved the raw dataset in artifact folder\")\n",
    "            \n",
    "            logging.info(\"here i have performed train test split\")\n",
    "            \n",
    "            train_data,test_data=train_test_split(data,test_size=0.25)\n",
    "            \n",
    "            \n",
    "            logging.info(\"train test split completed\")\n",
    "            \n",
    "            train_data.to_csv(self.ingestion_config.train_data_path,index=False)\n",
    "            test_data.to_csv(self.ingestion_config.test_data_path,index=False)\n",
    "            \n",
    "            logging.info(\"data ingestion part completed\")\n",
    "            \n",
    "            return (\n",
    "                 \n",
    "                \n",
    "                self.ingestion_config.train_data_path,\n",
    "                self.ingestion_config.test_data_path\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.info()\n",
    "            raise shippingException(e,sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a425c7fb-9c71-4a67-8d8d-6bdb80658524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.logger import logging\n",
    "from src.exception import shippingException\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder,StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from category_encoders.binary import BinaryEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "from src.utils.utils import save_object\n",
    "\n",
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "    preprocessor_obj_file_path=os.path.join('artifacts','preprocessor.pkl')\n",
    "\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self):\n",
    "        self.data_transformation_config=DataTransformationConfig()\n",
    "\n",
    "\n",
    "\n",
    "    def get_data_transformation(self):\n",
    "        \n",
    "        try:\n",
    "            logging.info('Data Transformation initiated')\n",
    "\n",
    "            numerical_cols=['unit_of_measure_(per_pack)','line_item_quantity','pack_price',\n",
    "                            'unit_price',\n",
    "                            'line_item_insurance_(usd)','freight_cost_(usd)']\n",
    "            \n",
    "            categorical_cols=['fulfill_via','vendor_inco_term','shipment_mode','first_line_designation']\n",
    "\n",
    "            \n",
    "            ## Numerical Pipeline\n",
    "            num_pipeline=Pipeline(\n",
    "                steps=[\n",
    "                ('scaler',StandardScaler())\n",
    "                ])\n",
    "            \n",
    "            # Categorigal Pipeline\n",
    "            cat_pipeline=Pipeline(\n",
    "                steps=[\n",
    "                ('imputer',SimpleImputer(strategy='most_frequent')),\n",
    "                ('onehotencoder', OneHotEncoder())\n",
    "                ])\n",
    "            \n",
    "            preprocessor=ColumnTransformer([\n",
    "            ('num_pipeline',num_pipeline,numerical_cols),\n",
    "            ('cat_pipeline',cat_pipeline,categorical_cols),\n",
    "            ])\n",
    "            \n",
    "            return preprocessor\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.info(\"Exception occured in the initiate_datatransformation\")\n",
    "\n",
    "            raise shippingException(e,sys)\n",
    "    \n",
    "    def _outlier_capping(self,df,col):\n",
    "        \"\"\"\n",
    "        Method Name :   _outlier_capping\n",
    "\n",
    "        Description :   This method performs outlier capping in the dataframe. \n",
    "\n",
    "        Output      :   DataFrame. \n",
    "        \"\"\"\n",
    "        logging.info(\"Entered _outlier_capping method of Data_Transformation class\")\n",
    "        try:\n",
    "            logging.info(\"Performing _outlier_capping for columns in the dataframe\")\n",
    "            percentile25 = df[col].quantile(0.25)  # calculating 25 percentile\n",
    "            percentile75 = df[col].quantile(0.75)  # calculating 75 percentile\n",
    "\n",
    "            # Calculating upper limit and lower limit\n",
    "            iqr = percentile75 - percentile25\n",
    "            upper_limit = percentile75 + 1.5 * iqr\n",
    "            lower_limit = percentile25 - 1.5 * iqr\n",
    "\n",
    "            # Capping the outliers\n",
    "            df.loc[(df[col] > upper_limit), col] = upper_limit\n",
    "            df.loc[(df[col] < lower_limit), col] = lower_limit\n",
    "            logging.info(\n",
    "                \"Performed _outlier_capping method of Data_Transformation class\"\n",
    "            )\n",
    "\n",
    "            logging.info(\"Exited _outlier_capping method of Data_Transformation class\")\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            raise shippingException(e, sys) from e\n",
    "    ## To removing the irregularity in the freight cost data\n",
    "    @staticmethod\n",
    "    def _trans_freight_cost(x):\n",
    "                if x.find(\"See\")!=-1:\n",
    "                    return np.nan\n",
    "                elif x==\"Freight Included in Commodity Cost\" or x==\"Invoiced Separately\":\n",
    "                    return 0\n",
    "                else:\n",
    "                    return x\n",
    "            \n",
    "    \n",
    "    def initialize_data_transformation(self,train_path,test_path):\n",
    "\n",
    "        try:\n",
    "            self.train_df=pd.read_csv(train_path)\n",
    "            self.test_df=pd.read_csv(test_path)\n",
    "            \n",
    "            logging.info(\"read train and test data complete\")\n",
    "            logging.info(f'Train Dataframe Head : \\n{self.train_df.head().to_string()}')\n",
    "            logging.info(f'Test Dataframe Head : \\n{self.test_df.head().to_string()}')\n",
    "\n",
    "            print(\"Columns in train_df:\", self.train_df.columns)\n",
    "            \n",
    "            preprocessing_obj = self.get_data_transformation()\n",
    "\n",
    "            target_column_name='line_item_value'\n",
    "\n",
    "            drop_columns = [target_column_name,'id', 'project_code', 'pq_', 'po_/_so_', 'asn/dn_', 'country',\n",
    "                            'managed_by','pq_first_sent_to_client_date', 'po_sent_to_vendor_date','scheduled_delivery_date', \n",
    "                            'delivered_to_client_date','delivery_recorded_date', 'product_group', 'sub_classification',\n",
    "                            'vendor', 'item_description', 'molecule/test_type', 'brand', 'dosage','dosage_form',\n",
    "                            'manufacturing_site','weight_(kilograms)'\n",
    "                             ]\n",
    "            \n",
    "            \n",
    "            self.train_df[\"freight_cost_(usd)\"] = self.train_df[\"freight_cost_(usd)\"].apply(self._trans_freight_cost)\n",
    "            self.test_df[\"freight_cost_(usd)\"] = self.test_df[\"freight_cost_(usd)\"].apply(self._trans_freight_cost)\n",
    "            \n",
    "\n",
    "\n",
    "           \n",
    "            numerical_cols=['unit_of_measure_(per_pack)','line_item_quantity','line_item_value','pack_price',\n",
    "                            'unit_price',\n",
    "                            'line_item_insurance_(usd)','freight_cost_(usd)']\n",
    "            \n",
    "            for col in numerical_cols:\n",
    "                self.train_df[col] = self.train_df[col].fillna(self.train_df[col].median())\n",
    "                self.test_df[col] = self.test_df[col].fillna(self.test_df[col].median())\n",
    "                \n",
    "            self.train_df[\"freight_cost_(usd)\"]=self.train_df[\"freight_cost_(usd)\"].astype(\"float\")\n",
    "            self.test_df[\"freight_cost_(usd)\"]=self.test_df[\"freight_cost_(usd)\"].astype(\"float\")\n",
    "           \n",
    "                \n",
    "            \n",
    "                \n",
    "            logging.info(\"NaN values are being filled\") \n",
    "\n",
    "            # Outlier capping\n",
    "            logging.info(\"Got a list of numerical_col\")\n",
    "            [self._outlier_capping(self.train_df,col) for col in numerical_cols]\n",
    "            logging.info(\"Outlier capped in train df\")\n",
    "            [self._outlier_capping(self.test_df,col) for col in numerical_cols]\n",
    "            logging.info(\"Outlier capped in test df\")\n",
    "\n",
    "            input_feature_train_df = self.train_df.drop(columns=drop_columns,axis=1)\n",
    "            target_feature_train_df=self.train_df[target_column_name]\n",
    "            \n",
    "            input_feature_test_df=self.test_df.drop(columns=drop_columns,axis=1)\n",
    "            target_feature_test_df=self.test_df[target_column_name]\n",
    "\n",
    "            input_feature_train_arr=preprocessing_obj.fit_transform(input_feature_train_df)\n",
    "            \n",
    "            \n",
    "            input_feature_test_arr=preprocessing_obj.transform(input_feature_test_df)\n",
    "            \n",
    "\n",
    "            train_arr = np.c_[input_feature_train_arr, np.array(target_feature_train_df)]\n",
    "            test_arr = np.c_[input_feature_test_arr, np.array(target_feature_test_df)]\n",
    "\n",
    "            save_object(\n",
    "                file_path=self.data_transformation_config.preprocessor_obj_file_path,\n",
    "                obj=preprocessing_obj\n",
    "            )\n",
    "            \n",
    "            logging.info(\"preprocessing pickle file saved\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            return (\n",
    "                train_arr,\n",
    "                test_arr\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.info(\"Exception occured in the initiate_datatransformation\")\n",
    "\n",
    "            raise shippingException(e,sys)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4efef4c0-5766-47bb-957d-9a07e80ee610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import importlib\n",
    "import numpy as np\n",
    "from src.logger import logging\n",
    "from src.exception import shippingException\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "from src.utils.utils import save_object\n",
    "from src.utils.utils import load_object\n",
    "from src.utils.utils import evaluate_model\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge,Lasso,ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "@dataclass\n",
    "class ModelTrainerConfig:\n",
    "    trained_model_file_path = os.path.join('artifacts','model.pkl')\n",
    "    \n",
    "    \n",
    "class ModelTrainer:\n",
    "    def __init__(self):\n",
    "        self.model_trainer_config = ModelTrainerConfig()\n",
    "    \n",
    "    def initate_model_training(self,train_array,test_array):\n",
    "        try:\n",
    "            logging.info('Splitting Dependent and Independent variables from train and test data')\n",
    "            X_train, y_train, X_test, y_test = (\n",
    "                train_array[:,:-1],\n",
    "                train_array[:,-1],\n",
    "                test_array[:,:-1],\n",
    "                test_array[:,-1]\n",
    "            )\n",
    "            \n",
    "            print(train_array)\n",
    "\n",
    "            models={\n",
    "            'LinearRegression':LinearRegression(),\n",
    "            'Lasso':Lasso(),\n",
    "            'Ridge':Ridge(),\n",
    "            'Elasticnet':ElasticNet(),\n",
    "            'Randomforest':RandomForestRegressor(n_estimators=100)\n",
    "            ##'XGBOOST': XGBRegressor()\n",
    "            }\n",
    "\n",
    "            \n",
    "            \n",
    "                \n",
    "            \n",
    "        \n",
    "            \n",
    "            model_report:dict=evaluate_model(X_train,y_train,X_test,y_test,models)\n",
    "            print(model_report)\n",
    "            print('\\n====================================================================================\\n')\n",
    "            logging.info(f'Model Report : {model_report}')\n",
    "\n",
    "            # To get best model score from dictionary \n",
    "            best_model_score = max(sorted(model_report.values()))\n",
    "\n",
    "            best_model_name = list(model_report.keys())[\n",
    "                list(model_report.values()).index(best_model_score)\n",
    "            ]\n",
    "            \n",
    "            best_model = models[best_model_name]\n",
    "            \n",
    "\n",
    "            print(f'Best Model Found, Model Name : {best_model_name} , R2 Score : {best_model_score}')\n",
    "            print('\\n====================================================================================\\n')\n",
    "            logging.info(f'Best Model Found , Model Name : {best_model_name} , R2 Score : {best_model_score}')\n",
    "\n",
    "            save_object(\n",
    "                 file_path=self.model_trainer_config.trained_model_file_path,\n",
    "                 obj=best_model\n",
    "            )\n",
    "          \n",
    "\n",
    "        except Exception as e:\n",
    "            logging.info('Exception occured at Model Training')\n",
    "            raise shippingException(e,sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac706a04-badc-4351-ba6d-42237df77bf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in train_df: Index(['id', 'project_code', 'pq_', 'po_/_so_', 'asn/dn_', 'country',\n",
      "       'managed_by', 'fulfill_via', 'vendor_inco_term', 'shipment_mode',\n",
      "       'pq_first_sent_to_client_date', 'po_sent_to_vendor_date',\n",
      "       'scheduled_delivery_date', 'delivered_to_client_date',\n",
      "       'delivery_recorded_date', 'product_group', 'sub_classification',\n",
      "       'vendor', 'item_description', 'molecule/test_type', 'brand', 'dosage',\n",
      "       'dosage_form', 'unit_of_measure_(per_pack)', 'line_item_quantity',\n",
      "       'line_item_value', 'pack_price', 'unit_price', 'manufacturing_site',\n",
      "       'first_line_designation', 'weight_(kilograms)', 'freight_cost_(usd)',\n",
      "       'line_item_insurance_(usd)'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data_transformation=DataTransformation()\n",
    "\n",
    "train_arr,test_arr=data_transformation.initialize_data_transformation(train_data_path,test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2304714e-dcd6-43bf-b782-40eb75cbc832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from src.logger import logging\n",
    "from src.exception import shippingException\n",
    "\n",
    "\n",
    "from src.components.ingestion import DataIngestion\n",
    "from src.components.transformation import DataTransformation\n",
    "from src.components.training import ModelTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "292ee9d2-43ce-4805-ae4a-b68f11602276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LinearRegression': 0.9516215081504077, 'Lasso': 0.9516335003302373, 'Ridge': 0.9516256514333578, 'Elasticnet': 0.876760339653761, 'XGBOOST': 0.9530401700216486}\n",
      "\n",
      "====================================================================================\n",
      "\n",
      "Best Model Found, Model Name : XGBOOST , R2 Score : 0.9530401700216486\n",
      "\n",
      "====================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_trainer_obj=ModelTrainer()\n",
    "model_trainer_obj.initate_model_training(train_arr,test_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ad248c39-95da-472f-a0d1-8c3dd8e38f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ab994b-3bb5-442a-ae64-2cca0f08542e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af399de-cfba-4b30-b85c-42707e203cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
